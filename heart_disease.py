# -*- coding: utf-8 -*-
"""S_001_heart_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17545ZbjQU3lBatWTsI-RAWFltBzT_cBw
"""

# Required library and line of code to connect to Google Drive.
# If you are working on a local computer, you can skip this part.
from google.colab import drive
drive.mount('/content/drive')

# Ahmet Yıldız
import pandas as pd
import numpy as np
import tensorflow.keras as k
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import os
# The path to the file will be working with.
os.chdir('/content/drive/MyDrive/yapay_zeka_video/S-001-Heart-Disease')

# Dataset has been downloaded from https://www.kaggle.com/ronitf/heart-disease-uci.

# Read the data
df=pd.read_csv('heart.csv')
df.head(10)

# Which columns are in the dataset?
# Is there a null or invalid value?
# What are the data types of the columns?
df.info()

# There is no string type in the dtype part. can display it graphically or 
# perform normalization without any additional processing.
# The 'target' column has values 0 and 1. If there were values such as 
# 'Has heart disease' and 'No heart disease' instead of these, 
# would have to convert them to numbers.
# Or could have written 'Female' and 'Male' in the 'sex' column 
# instead of 0 or 1. In such a case, convert string values to numbers 
# by giving them ids to represent them.

# First, group the data according to their class. 
# Then graph each feature of the data.

class_0,class_1=[],[]
def split_class(colm):
  c0,c1=[],[]
  for x in range(len(df)):
    if df['target'][x]==0: c0.append(df[colm][x])
    else:c1.append(df[colm][x])
  return c0,c1

plt.figure(figsize=(20,50))

for x in range(len(df.columns)):
  class_0,class_1=split_class(df.columns[x])

  plt.subplot(14,1,x+1)
  plt.plot(class_0,'bo', markersize=4)
  plt.plot(class_1,'r+', markersize=4)
  plt.legend(['0','1'])
  plt.title(df.columns[x])
  plt.grid(which='both')
plt.tight_layout()
plt.show()

# Normalization
# Scale the data between 0-1. 
normalizer=MinMaxScaler()
data=normalizer.fit_transform(X=df.values.tolist())
# Rescaled data
df_data=pd.DataFrame(data,columns=df.columns)
df_data.head(10)

# Split data into input, output
input_size=13
x_data=data[:,:input_size]
y_data=data[:,input_size:]

print('x_data :',x_data.shape)
print('y_data :',y_data.shape)
print('-------------------')

# Data to be used in training: x and y train
# Data to be used in the test: x and y test
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.33, random_state=42)

print('x_train :',x_train.shape)
print('y_train :',y_train.shape)
print('x_test  :',x_test.shape)
print('y_test  :',y_test.shape)

dropout=0.2
# Let's create our model
# 1 input layer, number of units input_size=13
# 1 hidden layer, number of units 20, activation method 'relu'. 
#    Relu usually gives good results.
# 1 output layer, number of units 1, activation method 'sigmoid'.
#    trying to guess if there is only one class.

# And between layers add dropout to prevent overfitting.

model=k.Sequential()
model.add(k.layers.InputLayer(input_shape=(input_size,)))
model.add(k.layers.Dropout(dropout))
model.add(k.layers.Dense(20,activation='relu'))
model.add(k.layers.Dropout(dropout))
model.add(k.layers.Dense(1,activation='sigmoid'))

# optimizer : 'Adam'. It generally gives good results.
# loss : 'binary_crossentropy'. It works best for classification problems 
#       that have only one class.
# metrics: 'accuracy' . This parameter is necessary for us to learn how much of 
#         the data is predicted correctly.

opt=k.optimizers.Adam(learning_rate=0.001)

model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])

model.summary()

#  train model
history=model.fit(x_train,y_train,batch_size=8,epochs=50,validation_data=(x_test,y_test))

#  Let's show the training performance with a graph

plt.figure(figsize=(15,7))
plt.subplot(1,2,1)
plt.plot(history.history['loss'],'b-')
plt.plot(history.history['val_loss'],'r-')
plt.legend(['loss','val_loss'])
plt.grid(which='both')

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'],'b-')
plt.plot(history.history['val_accuracy'],'r-')
plt.legend(['accuracy','val_accuracy'])
plt.grid(which='both')

plt.tight_layout()
plt.show()

# see the performance of our model with the evalute method

train_score=model.evaluate(x_train,y_train,verbose=0)

test_score=model.evaluate(x_test,y_test,verbose=0)

print('Trian     Loss : ',train_score[0])
print('Trian Accuracy :%',train_score[1]*100)
print()
print('Test      Loss : ',test_score[0])
print('Test  Accuracy :%',test_score[1]*100)

# save model
model.save('s_001_heart_disease.h5')

# test it manually

# age-sex-cp-trestbps-chol-fbs-restecg-thalach-exang-oldpeak-slope-ca-thal-target
# 70	1	1	156	245	0	0	143	0	0	2	0	2	1
test_data=[70	,1,	1	,156,	245,	0	,0,	143,	0	,0	,2,	0,	2	,1]

# apply normalization
test_data=normalizer.transform([test_data])
pred_test_data=model.predict(test_data[:,:input_size])
pred_test_data[0][0]

def get_input(msg):
  while True:
    v=input(msg)
    if v.isdecimal():
      return float(v)
    else:
      if v=='q':
        return 'q'
      else:
        print("Please enter a valid value.")

# load model from file

model=k.models.load_model("s_001_heart_disease.h5")

isContinue=True
while isContinue:
  print("press q to exit")
  test_data=[]
  for x in range(len(df.columns)-1):
    v=get_input(df.columns[x]+" :")
    if v=='q':
      isContinue=False
      break
    else:
      test_data.append(v)
  
  if isContinue:
    test_data.append(1)
    test_data=normalizer.transform([test_data])
    pred_test_data=model.predict(test_data[:,:input_size])
    if pred_test_data[0][0]<0.5:
      print("Has heart disease.")
    else:
      print("No heart disease.")
    
    print(pred_test_data[0][0],"\n")

# 70	1	1	156	245	0	0	143	0	0	2	0	2	1